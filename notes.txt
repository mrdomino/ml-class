Week 1, Day 1

Starting with ``Welcome''

- Supervised learning: classification, regression problems; dataset includes
  labels or "right" answers already.
- Unsupervised learning: clustering; determining structure in data.

Variables used throughout course:

- $m$: number of items in dataset
- $y$: the answer, output of function
- $x$: inputs to function
- $h$: $x -> y$, hypothesis function
- $\theta$: coefficients of regression hypothesis function

    Single-variable linear regression
aka univariate      "      "

Single-variable: single input. $x$ is a 1d vector.

Find $h_\theta$ that's closest to the results.

That is, find $\theta_0, \theta_1$ in $h_\theta = \theta_0 + \theta_1 x$ so that
cost function
$J_{\theta_0, \theta_1} =
    1/2m \sum_{i=0}^m (h_\theta(x^(i)) - y)^2$
is minimal.


Week 1, Day 2

Starting with ``Cost Function - Intuition I''

Hypothesis function $h_\theta(x)$, given fixed $\theta = \theta_0, \theta_1$,
is a function of $x$. Cost function $J(\theta)$ is a function of $\theta$. With
1d $\theta=\theta_1$ and $h_\theta(x) = \theta_1 x$, $J$ looks like a parabola
with its lowest point at the optimal value of $\theta$ to fit the data.

With 2d $\theta=\theta_0, \theta_1$, cost function looks like a bowl with
minimum at the lowest point.

Contour plots: 2 input variables, lines at fixed output values

## Gradient Descent

Start with some $\theta_0, \theta_1$,
move in the direction
that makes $J(\theta_0, \theta_1)$ smaller,
hopefully reach some minimum.

Precisely:

$\alpha$ is a scaling factor---larger values mean larger steps.
$\deriv_j J(\theta_0, \theta_1)$ is derivative of $J$ wrt $\theta_j$.

repeat until convergence {
  $\theta_j := -\alpha \deriv_j J(\theta_0, \theta_1)$
}

Simultaneous update: compute right-hand side for all $j$, then do assigns. Don't
use updated values in subsequent assigns; that's not gradient descent. It turns
out to actually usually work, but it has stranger properties, and isn't what
people are referring to.


Week 1, Day 3

Starting with ``Gradient Descent Intuition''

Gradient Descent takes the derivative of the cost function and applies a
negative scaling factor. The derivative gives us the slope, so inverting it
points us downards on the function; the scaling factor gives us movement in the
direction of descent.

For a fixed $\alpha$, gradient descent will take smaller steps as it gets closer
to a local optimum because the slope will get smaller.


Our specific cost function
$J(\theta) = 1/(2m) \sum_{i=0}^m (h_\theta(x^(i)) - y^(i))^2$
has partial derivatives
$\deriv_0 J(\theta) = 1/m \sum_{i=0}^m h_\theta(x^(i)) - y^(i)$
$\deriv_1 J(\theta) = 1/m \sum_{i=0}^m (h_\theta(x^(i)) - y^(i)) x^(i)$
and happens to be convex, so gradient descent will always converge on the global
optimum.

This gives us Gradient Descent for Linear Regression.


Two extensions of linear regression:

1. Minimize the cost function in one step rather than applying gradient descent
2. Work with larger numbers of features (e.g. house size, num bedrooms, ...).

Linear algebra comes in handy for this.
