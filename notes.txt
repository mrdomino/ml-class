vim:ai:tw=80:ts=2:sw=2:et

Week 1, Day 1

Starting with ``Welcome''

- Supervised learning: classification, regression problems; dataset includes
  labels or "right" answers already.
- Unsupervised learning: clustering; determining structure in data.

Variables used throughout course:

- $m$: number of items in dataset
- $y$: the answer, output of function
- $x$: inputs to function
- $h$: $x -> y$, hypothesis function
- $\theta$: coefficients of regression hypothesis function

    Single-variable linear regression
aka univariate      "      "

Single-variable: single input. $x$ is a 1d vector.

Find $h_\theta$ that's closest to the results.

That is, find $\theta_0, \theta_1$ in $h_\theta = \theta_0 + \theta_1 x$ so that
cost function
$J_{\theta_0, \theta_1} =
    1/2m \sum_{i=0}^m (h_\theta(x^(i)) - y)^2$
is minimal.


Week 1, Day 2

Starting with ``Cost Function - Intuition I''

Hypothesis function $h_\theta(x)$, given fixed $\theta = \theta_0, \theta_1$,
is a function of $x$. Cost function $J(\theta)$ is a function of $\theta$. With
1d $\theta=\theta_1$ and $h_\theta(x) = \theta_1 x$, $J$ looks like a parabola
with its lowest point at the optimal value of $\theta$ to fit the data.

With 2d $\theta=\theta_0, \theta_1$, cost function looks like a bowl with
minimum at the lowest point.

Contour plots: 2 input variables, lines at fixed output values

## Gradient Descent

Start with some $\theta_0, \theta_1$,
move in the direction
that makes $J(\theta_0, \theta_1)$ smaller,
hopefully reach some minimum.

Precisely:

$\alpha$ is a scaling factor---larger values mean larger steps.
$\deriv_j J(\theta_0, \theta_1)$ is derivative of $J$ wrt $\theta_j$.

repeat until convergence {
  $\theta_j := -\alpha \deriv_j J(\theta_0, \theta_1)$
}

Simultaneous update: compute right-hand side for all $j$, then do assigns. Don't
use updated values in subsequent assigns; that's not gradient descent. It turns
out to actually usually work, but it has stranger properties, and isn't what
people are referring to.


Week 1, Day 3

Starting with ``Gradient Descent Intuition''

Gradient Descent takes the derivative of the cost function and applies a
negative scaling factor. The derivative gives us the slope, so inverting it
points us downards on the function; the scaling factor gives us movement in the
direction of descent.

For a fixed $\alpha$, gradient descent will take smaller steps as it gets closer
to a local optimum because the slope will get smaller.


Our specific cost function
$J(\theta) = 1/(2m) \sum_{i=0}^m (h_\theta(x^(i)) - y^(i))^2$
has partial derivatives
$\deriv_0 J(\theta) = 1/m \sum_{i=0}^m h_\theta(x^(i)) - y^(i)$
$\deriv_1 J(\theta) = 1/m \sum_{i=0}^m (h_\theta(x^(i)) - y^(i)) x^(i)$
and happens to be convex, so gradient descent will always converge on the global
optimum.

This gives us Gradient Descent for Linear Regression.


Two extensions of linear regression:

1. Minimize the cost function in one step rather than applying gradient descent
2. Work with larger numbers of features (e.g. house size, num bedrooms, ...).

Linear algebra comes in handy for this.


Week 2, Day 1

Multiple features gives us

## Multivariate Linear Regression

where we can treat x_1, ..., x_n features as a column vector. If we add x_0 = 1
to the vector, we get an n+1 x 1 vector, and our hypothesis function
$h_\theta(x)$ is now simply $\theta^T x$.

Now $J(\theta)$ is a function of the feature vectors.

Our update rule is:

$\theta_j := \theta_j - \alpha \deriv_(\theta_j) J(\theta)$,
which still gives us the values we want in the single-variable class:
$\theta_j := \theta_j - \alpha 1/m \sum_i=1^m (h_\theta(x^(i)) - y^(i)) x^(i)$.

## Feature scaling

In practice, gradient descent is much faster if all its features lie in
approximately the same range, e.g. $-1 \le x \le 1$. We may also want to
normalize to the mean of $x$.

## Learning rate

Too big: won't converge. Too small: converges slowly. Just right: big as
possible before breaking.


Week 2, Day 4

Features and polynomial regression

let features be functions of other features. Done.


Normal Equation

$\theta = (X^T X)^(-1) X^T y$ gives us optimal theta values, but since computing
inverses is O(n^3), it doesn't work as well for large n and we might want to use
gradient descent instead.


Octave

+ - * / ^
== ~=
&& || xor()

ones
zeros
eye
rand

size(A) => [n p]

size(A, 1) => n
size(A, 2) => p

who: variables

whos: variables with details

load: load file into environment

clear: remove var from env

save: binary, save -ascii => readable

A(:, 2) => 2nd column
A(2, :) => 2nd row

A([2 3], :) => 2nd and 3rd rows

A = [A, [1; 3; 4]] => append column vector to A

A(:) => all elems into single column vector

[A, B] == [A B]


A *  B: matrix mul
A .* B: element-wise mul

A .^ 2: element-wise square

1 ./ A: element-wise recip

log(A): elem-wise log
exp(A)
abs(A)
-A

v + ones(length(v), 1) == v + 1

A': transpose

(A')' == A

max(A): max val
[val, ind] = max(A)

a < 3: elem-wise compare

find(a < 3): row vector of indices

A = magic(3): magic square

sum(a): sum of vector
prod(a): prod of vector
floor(a): round down
ceil

max(rand(3), rand(3))

max(A, [], 1) => column-wise max
max(A, [], 2) => row-wise

max(max(A)) => max elem


Week 3

Logistic regression

Classification recap: $y\in{0,1}$. Linear regression doesn't work so well.

Logistic regression gives us a hypothesis such that $0 \le h_\theta(x) \le 1$.

For linear regression, we used $h_\theta(x) = \theta^T x$.
For logistic regression, we use $h_\theta(x) = g(\theta^T x)$,
where $g(z) = 1/(1 + e^{-z})$, the sigmoid (or logistic) function.

The hypothesis $h_\theta(x)$ corresponds to our guess at $P(y=1|x;\theta)$, the
probability that $y=1$ given $x$ parameterized by $\theta$.

Since $y$ is binary, $P(y=0|x;\theta)$ is simply $1 - P(y=1|...)$.


Decision boundary


Cost function

J(\theta) = 1/m sum cost

cost = -log(h(x)) if y=1
       -log(1 - h(x)) if y=0


Simplified Cost Function and Gradient Descent

Read "Maximum Likelihood Estimation" for more details.

Gradient descent: same algorithm, different hypothesis function

Vectorized form:

$\theta = \theta - \alpha \sum_{i=1}^m [(h_\theta(x^(i)) - y^(i)) x^(i)]$


Advanced Optimization

Other algorithms better than gradient descent:

- Conjugate gradient
- BFGS
- L-BFGS

Don't need manually picked $\alpha$.

Often faster, but more complex.

Usage, e.g.:

    [jVal, gradient] = costFunction(theta)

Call with `fminunc`.


Multi-class classification problems

One-vs-all classification

General idea: train classifiers per classes. For new input, pick the
classifier that is maximal valued.


## Regularization

Problem of overfitting

Overfitting: given too many features, the hypothesis can fit the training set
well, but fail to generalize to new examples.

Addressing overfitting:

1. Reduce number of features.
  - Manually select which features to keep.
  - Model selection algorithm (later in course).
2. Regularization.
  - Keep all the features, but reduce the magnitude of the parameters.
  - Works well when we have a lot of features, each of which contributes a bit
    to predicting y.


Cost function

Small values found by adding magnitude of parameter to cost function.

Penalizing values gives ``simpler'' hypotheses, less prone to overfitting.


Regularized Linear Regression

Add a term of $\lambda \sum_j=1^n \theta_j^2$.

Regularized normal eqn: $(X^T X + \lambda mat)^{-1} X^T y$ where $mat$ has zeros
on all diagonals except at $(0,0)$.


Week 4

Neural networks

Regression algorithms suck with large numbers of features that are nonlinear.


Brain learning: "one learning algorithm" hypothesis

You can cut communication to chunks of the brain off and rewire them, and cause
the auditory cortex or the somatosensory cortex to learn to see.


Described networks use sigmoid function for neural activation


Input layer, hidden layer, output layer

Layers numbered from input (1) onwards towards output

Terminology:

$a_i^(j)$ is the ``activation'' of unit (neuron) $i$ in layer $j$.

$\Theta^(j)$ is the matrix of weights controlling the function mapping layer $j$
to layer $j+1$.

Suppose $s_j$ neurons in layer j, $s_{j+1}$ in layer $j+1$, then $\Theta^(j)$
has dimension $s_{j+1} \times (s_j + 1)$. That is, one row for each neuron in
the next layer, and one column for each neuron in the current layer (with one
extra for the bias unit).

The bias unit is a constant 1; it is independent of the input, so it can be used
to weight the overall output by some amount based on the weight assigned to its
connection.

It turns out that you can think of this type of neural network as just iterated
logistic regression. Suppose you have one output unit $a_1^(3)$. It's computed
as $a_1^(3) = g(\Theta^(2) a^(2))$. Then $\Theta^(2)$ is our feature set and
$a^(2)$ is our input vector.


Aside: XNOR simpler to learn than XOR.


Computing AND with a 2-layer network: set bias weight $\Theta_0^(1) = -30$,
$\Theta_1^(1) = 20$, $\Theta_2^(1) = 20$, and you have
$a^(2) = g(-30 + 20 x_1 + 20 x_2)$, which is close to 1 for $x_1$ and $x_2$ both
1, and close to 0 otherwise.


XNOR can be computed with a neural net with one hidden layer: first hidden node
computes (NOT x1) AND (NOT x2), second hidden node computes x1 AND x2, output
node computes (hidden_1 OR hidden_2).


Week 5

Neural network learning

For neural networks, the cost function $J(\Theta)$ is like the cost function for
logistic regression, except we're also summing for each output node and
regularizing for each $\Theta_{i,j}^(l)$ ($i$th node of current layer, $j$th
node of next layer, $l$th layer in the network.) So (assuming $K$ output nodes):

$J(\Theta) = \sum_{i=1}^m \sum_{k=1}^K y_k^(i) \log(h_\Theta(x^(i)))_k
                                     + (1 - y_k^(i) ...$

Backpropagation: feed-forward but in reverse. Compute the error term
$\delta^(L) = y^(L) - a^(L)$. Compute $\delta$ for the other layers. This gives
us the gradient for $\Theta$, but it's complicated to show exactly how.


Gradient checking: Numerical approximation of $\deriv J$ by
$(J(\theta + \epsilon) - J(\theta - \epsilon)) / 2\epsilon$. Use this to verify
that backpropagation is working correctly.


Week 6

Evaluating a hypothesis: split your data into a training set and test set. If
the error is low on the training set and high on the test set, your hypothesis
is probably overfitting.

For selecting polynomial features, split into three groups: training,
validation, and test. Select the degree of polynomial that minimizes cost on the
validation set. Report generalization error based on the test set. (Otherwise
the test set will have been fit to the degree of the polynomial so the cost will
be underestimated.)


High bias: your curve doesn't fit the training or test data properly
(underfitting).

High variance: your curve overfits the training data, failing to predict the
test data.

Adding more examples can help with high variance, but probably won't help with
high bias.


Week 6

Prioritizing work: get something quick and dirty running, then refine it.

Use error analysis to test hypotheses numerically. That is: look at errors your
algorithm makes on the cross-validation set. For those errors, see if you can
come up with modifications to the algorithm that'd improve performance on them.
Classify errors into different types (e.g. for spam: pharma, phishing,
discounts, ...) and see if some type has more than the others.

Use a numerical metric to test experiments -- if you think something will
improve performance, test the error rate with and without it.

Pure error rate can be misleading for skewed classes (e.g. where y = 0 almost
always, like for a cancer test). In those cases, there are better metrics that
can be used based on the rate of false positives or false negatives vs. true
positives/negatives. These are ``precision'' and ``recall''.

Precision: true positives / predicted positives, or
           true positives / (true positives + false positives)

Recall: true positives / actual positives, or
        true positives / (true positives + false negatives)

It's impossible for a learning algorithm that isn't learning (e.g. predicting y
= 0 constantly) to have both high precision and high recall, so these can give a
better sense for actual performance.

Deciding between precision and recall can be tricky. Taking the average of the
two is a poor idea since it removes the benefit of using them as opposed to just
pure error rate. Instead, you can use the F score: $2 PR / (P + R)$. The higher,
the better.


General rule of thumb: more data is better.


Week 7

Support vector machines

Can be thought of as a variant on logistic regression but with a different cost
function. Instead of terms $C_1 = -log g(z)$ and $C_2 = -log (1 - g(z))$ (where
$C_1 is small when $\theta^T x > 0$ and $C_2$ is small when $\theta^T x < 0$),
we set $C_1 = 0$ whenever $z = \theta^T x >= 1$ and linearly increasing as $z$
decreases, and $C_2 = 0$ whenever $z = \theta^T x <= -1$ and linearly increasing
as $z$ increases.

We also use different parameterization for the cost function. We drop the $1/m$
term. For regularization, if $J = A + \lambda / 2 B$ where $B$ is the
regularization term, we use $C$ instead of $\lambda$: $J = C A + 1/2 B$. If we
set $C = 1 / \lambda$, both cost functions will find us the same $\theta$.


SVMs are sometimes called ``large margin classifiers'' because they produce
hypotheses that have large distance between the decision boundary line and any
examples. (This was very visual -- challenging to explain in text; recommend
watching ``Large Margin Intuition''.)
